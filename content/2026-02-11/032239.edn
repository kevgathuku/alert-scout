[{:rule-id "rule-ai", :item {:feed-id "hn-frontpage", :item-id "https://news.ycombinator.com/item?id=46962641", :title "Show HN: Rowboat – AI coworker that turns your work into a knowledge graph (OSS)", :link "https://github.com/rowboatlabs/rowboat", :published-at #inst "2026-02-10T16:47:29.000-00:00", :content "Hi HN, AI agents that can run tools on your machine are powerful for knowledge work, but they’re only as useful as the context they have. Rowboat is an open-source, local-first app that turns your work into a living knowledge graph (stored as plain Markdown with backlinks) and uses it to accomplish tasks on your computer. For example, you can say \"Build me a deck about our next quarter roadmap.\" Rowboat pulls priorities and commitments from your graph, loads a presentation skill, and exports a PDF. Our repo is https://github.com/rowboatlabs/rowboat, and there’s a demo video here: https://www.youtube.com/watch?v=5AWoGo-L16I Rowboat has two parts: (1) A living context graph: Rowboat connects to sources like Gmail and meeting notes like Granola and Fireflies, extracts decisions, commitments, deadlines, and relationships, and writes them locally as linked and editable Markdown files (Obsidian-style), organized around people, projects, and topics. As new conversations happen (including voice memos), related notes update automatically. If a deadline changes in a standup, it links back to the original commitment and updates it. (2) A local assistant: On top of that graph, Rowboat includes an agent with local shell access and MCP support, so it can use your existing context to actually do work on your machine. It can act on demand or run scheduled background tasks. Example: “Prep me for my meeting with John and create a short voice brief.” It pulls relevant context from your graph and can generate an audio note via an MCP tool like ElevenLabs. Why not just search transcripts? Passing gigabytes of email, docs, and calls directly to an AI agent is slow and lossy. And search only answers the questions you think to ask. A system that accumulates context over time can track decisions, commitments, and relationships across conversations, and surface patterns you didn't know to look for. Rowboat is Apache-2.0 licensed, works with any LLM (including local ones), and stores all data locally as Markdown you can read, edit, or delete at any time. Our previous startup was acquired by Coinbase, where part of my work involved graph neural networks. We're excited to be working with graph-based systems again. Work memory feels like the missing layer for agents. We’d love to hear your thoughts and welcome contributions! Comments URL: https://news.ycombinator.com/item?id=46962641 Points: 121 # Comments: 30"}, :excerpts [{:text "... Rowboat is Apache-2.0 licensed, works with any LLM (including local ones), and stores all data...", :matched-terms ["llm"], :source :content}]} {:rule-id "rule-ai", :item {:feed-id "hn-frontpage", :item-id "https://news.ycombinator.com/item?id=46965012", :title "Show HN: Multimodal perception system for real-time conversation", :link "https://raven.tavuslabs.org", :published-at #inst "2026-02-10T18:58:35.000-00:00", :content "I work on real-time voice/video AI at Tavus and for the past few years, I’ve mostly focused on how machines respond in a conversation. One thing that’s always bothered me is that almost all conversational systems still reduce everything to transcripts, and throw away a ton of signals that need to be used downstream. Some existing emotion understanding models try to analyze and classify into small sets of arbitrary boxes, but they either aren’t fast / rich enough to do this with conviction in real-time. So I built a multimodal perception system which gives us a way to encode visual and audio conversational signals and have them translated into natural language by aligning a small LLM on these signals, such that the agent can \"see\" and \"hear\" you, and that you can interface with it via an OpenAI compatible tool schema in a live conversation. It outputs short natural language descriptions of what’s going on in the interaction - things like uncertainty building, sarcasm, disengagement, or even shift in attention of a single turn in a convo. Some quick specs: - Runs in real-time per conversation - Processing at ~15fps video + overlapping audio alongside the conversation - Handles nuanced emotions, whispers vs shouts - Trained on synthetic + internal convo data Happy to answer questions or go deeper on architecture/tradeoffs More details here: https://www.tavus.io/post/raven-1-bringing-emotional-intelli... Comments URL: https://news.ycombinator.com/item?id=46965012 Points: 42 # Comments: 12"}, :excerpts [{:text "... into natural language by aligning a small LLM on these signals, such that the agent can \"see\"...", :matched-terms ["llm"], :source :content}]}]