[{:rule-id "rule-ai", :item {:feed-id "hn-frontpage", :item-id "https://news.ycombinator.com/item?id=46641362", :title "Show HN: Gambit, an open-source agent harness for building reliable AI agents", :link "https://github.com/bolt-foundry/gambit", :published-at #inst "2026-01-16T00:13:25.000-00:00", :content "\n<p>Hey HN!<p>Wanted to show our open source agent harness called Gambit.<p>If you’re not familiar, agent harnesses are sort of like an operating system for an agent... they handle tool calling, planning, context window management, and don’t require as much developer orchestration.<p>Normally you might see an agent orchestration framework pipeline like:<p>compute -> compute -> compute -> LLM -> compute -> compute -> LLM<p>we invert this so with an agent harness, it’s more like:<p>LLM -> LLM -> LLM -> compute -> LLM -> LLM -> compute -> LLM<p>Essentially you describe each agent in either a self contained markdown file, or as a typescript program. Your root agent can bring in other agents as needed, and we create a typesafe way for you to define the interfaces between those agents. We call these decks.<p>Agents can call agents, and each agent can be designed with whatever model params make sense for your task.<p>Additionally, each step of the chain gets automatic evals, we call graders. A grader is another deck type… but it’s designed to evaluate and score conversations (or individual conversation turns).<p>We also have test agents you can define on a deck-by-deck basis, that are designed to mimic scenarios your agent would face and generate synthetic data for either humans or graders to grade.<p>Prior to Gambit, we had built an LLM based video editor, and we weren’t happy with the results, which is what brought us down this path of improving inference time LLM quality.<p>We know it’s missing some obvious parts, but we wanted to get this out there to see how it could help people or start conversations. We’re really happy with how it’s working with some of our early design partners, and we think it’s a way to implement a lot of interesting applications:<p>- Truly open source agents and assistants, where logic, code, and prompts can be easily shared with the community.<p>- Rubric based grading to guarantee you (for instance) don’t leak PII accidentally<p>- Spin up a usable bot in minutes and have Codex or Claude Code use our command line runner / graders to build a first version that is pretty good w/ very little human intervention.<p>We’ll be around if ya’ll have any questions or thoughts. Thanks for checking us out!<p>Walkthrough video: <a href=\"https://youtu.be/J_hQ2L_yy60\" rel=\"nofollow\">https://youtu.be/J_hQ2L_yy60</a></p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46641362\">https://news.ycombinator.com/item?id=46641362</a></p>\n<p>Points: 32</p>\n<p># Comments: 7</p>\n"}, :excerpts [{:text "... like:<p>compute -> compute -> compute -> LLM -> compute -> compute -> LLM<p>we invert this so with an agent harness, it’s more like:<p>LLM -> LLM -> LLM -> compute -> LLM -> LLM -> compute -> LLM<p>Essentially you describe each agent in either a...", :matched-terms ["llm"], :source :content} {:text "... to grade.<p>Prior to Gambit, we had built an LLM based video editor, and we weren’t happy with the...", :matched-terms ["llm"], :source :content} {:text "... us down this path of improving inference time LLM quality.<p>We know it’s missing some obvious...", :matched-terms ["llm"], :source :content}]}]