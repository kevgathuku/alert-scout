[{:rule-id "rule-ai", :item {:feed-id "hn", :item-id "https://mage-bench.com/", :title "Show HN: I taught LLMs to play Magic: The Gathering against each other", :link "https://mage-bench.com/", :published-at #inst "2026-02-17T16:22:49.000-00:00", :content "Comments"}, :excerpts [{:text "Show HN: I taught LLMs to play Magic: The Gathering against each other", :matched-terms ["llm"], :source :title}]} {:rule-id "rule-ai", :item {:feed-id "hn", :item-id "https://www.anthropic.com/news/claude-sonnet-4-6", :title "Claude Sonnet 4.6", :link "https://www.anthropic.com/news/claude-sonnet-4-6", :published-at #inst "2026-02-17T17:48:52.000-00:00", :content "Comments"}, :excerpts [{:text "Claude Sonnet 4.6", :matched-terms ["claude"], :source :title}]} {:rule-id "rule-ai", :item {:feed-id "hn-frontpage", :item-id "https://news.ycombinator.com/item?id=47049776", :title "Launch HN: Sonarly (YC W26) – AI agent to triage and fix your production alerts", :link "https://sonarly.com/", :published-at #inst "2026-02-17T17:03:09.000-00:00", :content "Hey HN, I am Dimittri and we’re building Sonarly (https://sonarly.com), an AI engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here's a demo: https://www.youtube.com/watch?v=rr3VHv0eRdw. Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR. Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it's a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on. At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in production, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can't catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering). We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair). We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry. We wanted to build an interface where you don't need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the \"what,\" \"why,\" and \"how do I fix it.\" We quickly realized companies don't want to add a new tracker or change their monitoring stack, as these platforms do the job they're supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations. Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can’t simply pipe every alert directly into an agent. That’s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog/Grafana). However, things get exponentially harder with multi-repo and multi-service architectures. So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster. One of our users using Sentry was receiving ~180 alerts/day. Here is what their workflow looked like: - Receive the alert - 1) Defocus from their current task or wake up, or 2) don't look at the alert at all (most of the time) - Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc. - Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline) - Then fix by giving Claude Code the correct context We started by cutting the noise and went from 180/day to 50/day (by grouping issues) and giving a severity based on the impact on the user/infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA. We launched self-serve (https://sonarly.com) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts? We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I'll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything's constructive! Comments URL: https://news.ycombinator.com/item?id=47049776 Points: 10 # Comments: 0"}, :excerpts [{:text "... user feedback channels, and other integrations. Claude Code is so good at writing code, but handling...", :matched-terms ["claude"], :source :content} {:text "... Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the...", :matched-terms ["claude"], :source :content} {:text "... different services, logs, and metrics so that Claude Code can understand the issue faster. One of our...", :matched-terms ["claude"], :source :content}]} {:rule-id "rule-ai", :item {:feed-id "hn-frontpage", :item-id "https://news.ycombinator.com/item?id=47049848", :title "Show HN: Continue – Source-controlled AI checks, enforceable in CI", :link "https://docs.continue.dev", :published-at #inst "2026-02-17T17:08:14.000-00:00", :content "We now write most of our code with agents. For a while, PRs piled up, causing review fatigue, and we had this sinking feeling that standards were slipping. Consistency is tough at this volume. I’m sharing the solution we found, which has become our main product. Continue (https://docs.continue.dev) runs AI checks on every PR. Each check is a source-controlled markdown file in `.continue/checks/` that shows up as a GitHub status check. They run as full agents, not just reading the diff, but able to read/write files, run bash commands, and use a browser. If it finds something, the check fails with one click to accept a diff. Otherwise, it passes silently. Here’s one of ours:   .continue/checks/metrics-integrity.md\n\n  ---\n  name: Metrics Integrity\n  description: Detects changes that could inflate, deflate, or corrupt metrics (session counts, event accuracy, etc.)\n  ---\n\n  Review this PR for changes that could unintentionally distort metrics.\n  These bugs are insidious because they corrupt dashboards without triggering errors or test failures.\n\n  Check for:\n  - \"Find or create\" patterns where the \"find\" is too narrow, causing entity duplication (e.g. querying only active sessions, missing completed ones, so every new commit creates a duplicate)\n  - Event tracking calls inside loops or retry paths that fire multiple times per logical action\n  - Refactors that accidentally remove or move tracking calls to a path that executes with different frequency\n\n  Key files: anything containing `posthog.capture` or `trackEvent`\n\n This check passed without noise for weeks, but then caught a PR that would have silently deflated our session counts. We added it in the first place because we’d been burned in the past by bad data, only noticing when a dashboard looked off. --- To get started, paste this into Claude Code or your coding agent of choice:   Help me write checks for this codebase: https://continue.dev/walkthrough\n It will: - Explore the codebase and use the `gh` CLI to read past review comments - Write checks to `.continue/checks/` - Optionally, show you how to run them locally or in CI Would love your feedback! Comments URL: https://news.ycombinator.com/item?id=47049848 Points: 15 # Comments: 5"}, :excerpts [{:text "... looked off. --- To get started, paste this into Claude Code or your coding agent of choice:   Help me...", :matched-terms ["claude"], :source :content}]} {:rule-id "rule-ai", :item {:feed-id "hn-frontpage", :item-id "https://news.ycombinator.com/item?id=47050488", :title "Claude Sonnet 4.6 System Card", :link "https://www.anthropic.com/claude-sonnet-4-6-system-card", :published-at #inst "2026-02-17T17:48:52.000-00:00", :content "Article URL: https://www.anthropic.com/claude-sonnet-4-6-system-card Comments URL: https://news.ycombinator.com/item?id=47050488 Points: 69 # Comments: 26"}, :excerpts [{:text "Claude Sonnet 4.6 System Card", :matched-terms ["claude"], :source :title} {:text "Article URL: https://www.anthropic.com/claude-sonnet-4-6-system-card Comments URL:...", :matched-terms ["claude"], :source :content}]}]