[{:rule-id "rule-ai", :item {:feed-id "hn", :item-id "https://www.anthropic.com/research/assistant-axis", :title "The assistant axis: situating and stabilizing the character of LLMs", :link "https://www.anthropic.com/research/assistant-axis", :published-at #inst "2026-01-19T21:25:16.000-00:00", :content "<a href=\"https://news.ycombinator.com/item?id=46684708\">Comments</a>"}, :excerpts [{:text "... axis: situating and stabilizing the character of LLMs", :matched-terms ["llm"], :source :title}]} {:rule-id "rule-ai", :item {:feed-id "hn", :item-id "https://github.com/jordanhubbard/nanolang", :title "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", :link "https://github.com/jordanhubbard/nanolang", :published-at #inst "2026-01-19T21:48:07.000-00:00", :content "<a href=\"https://news.ycombinator.com/item?id=46684958\">Comments</a>"}, :excerpts [{:text "... language designed to be targeted by coding LLMs", :matched-terms ["llm"], :source :title}]} {:rule-id "rule-ai", :item {:feed-id "hn-frontpage", :item-id "https://news.ycombinator.com/item?id=46683858", :title "Show HN: A creative coding library for making art with desktop windows", :link "https://github.com/willmeyers/window-art", :published-at #inst "2026-01-19T20:11:25.000-00:00", :content "\n<p>Article URL: <a href=\"https://github.com/willmeyers/window-art\">https://github.com/willmeyers/window-art</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46683858\">https://news.ycombinator.com/item?id=46683858</a></p>\n<p>Points: 26</p>\n<p># Comments: 2</p>\n"}, :excerpts [{:text "\n<p>Article URL: <a...", :matched-terms ["llm"], :source :content} {:text "... URL: <a...", :matched-terms ["llm"], :source :content}]} {:rule-id "clojure-db", :item {:feed-id "planet-clojure", :item-id "tag:yyhh.org,2026-01-19:/blog/2026/01/triple-store-triple-progress-datalevin-posited-for-the-future", :title "Triple Store, Triple Progress: Datalevin Posited for the Future", :link "https://yyhh.org/blog/2026/01/triple-store-triple-progress-datalevin-posited-for-the-future", :published-at #inst "2026-01-19T16:00:00.000-00:00", :content "<p><img alt=\"Interactive Short Query Performance\" src=\"https://yyhh.org/images/interactive-short-query-performance.svg\" /></p>\n<p>In my previous post, <a href=\"https://yyhh.org/blog/2024/09/competing-for-the-job-with-a-triplestore/\">Competing for the JOB with a\nTriplestore</a>, I showed that a triple\nstore, such as <a href=\"https://github.com/juji-io/datalevin\">Datalevin</a>, can compete\nwith the best row stores on complex relational workloads. Since then, I have\nrewritten Datalevin's rule engine and improved its storage and query engine.\nThis post focuses on why these matter for the broader goal of using a triple\nstore as a single, flexible data substrate.</p>\n<h2>One store, many workloads</h2>\n<p>Our goal is to simplify data storage and access by supporting diverse database\nuse cases and paradigms, because maximal flexibility is the core strength of a\ntriple store. Using one data store for different use cases simplifies and\nreduces the cost of software development, deployment, and maintenance.</p>\n<p>Since 2020, we have been working hard toward this goal of building an easy-to-use\nand versatile database. I am happy to report that, today, in addition to\nkey-value and relational database features, Datalevin also handles graph queries\nand deductive logic reasoning tasks, with built-in support for full-text search\nand vector similarity search as well. All these are seamlessly integrated in a\ncompact package that works in both embedded and server modes.</p>\n<p>This post is a guided tour of the progress made so far in the storage engine,\nthe query engine, and the new rule engine.</p>\n<h2>Triples as the substrate</h2>\n<p>Datalevin stores data as triples: entity, attribute, value (EAV): the smallest,\natomic unit of a data item. This uniform representation is the key that lets one\nengine handle many shapes of data and many ways of asking questions. A triple\nstore can behave like a relational system when your data is tabular, like a\ngraph system when your data is connected, and like a logic system when you\ndefine recursive rules.</p>\n<p>The challenge has always been performance. Triple stores have historically been\nslower than row/column stores. The rest of this post explains how Datalevin\naddresses this challenge.</p>\n<h2>Storage</h2>\n<p>Datalevin uses a fast key-value database library as the storage layer.\nSpecifically, the exceptional read performance of\n<a href=\"https://en.wikipedia.org/wiki/Lightning_Memory-Mapped_Database\">LMDB</a> is the\nfoundation of Datalevin's query performance. Datalevin stores triples with\nnested indices by leveraging LMDB's DUPSORT capability: the head element of a\ntriple is stored once as the key, and the tail elements are stored as a sorted\nlist of values. This reduces storage overhead and alleviates the data redundancy\nproblem inherent in triple stores.</p>\n<p>To further reduce data redundancy, we built our own LMDB fork,\n<a href=\"https://github.com/huahaiy/dlmdb\">DLMDB</a>, which adds page-level prefix\ncompression to the storage. For DUPSORT, prefix compression is applied to both\nkeys and values, resulting in significant storage savings. We also removed the\nlesser-used <code>VAE</code> index. For a typical reference-heavy (foreign key) Datalog\ndatabase, the footprint reduction can be over 40%.</p>\n<p>Through relentless code optimization, we achieved these savings\nwithout incurring excessive read/write overhead. In fact, for the common\nDatalevin use case of seeking to a key and reading its list of values in full,\nwe obtained a 40% speedup in most cases.</p>\n<p>The Datalevin query planner relies heavily on online counting and sampling. To\nfacilitate these operations, we added subtree node count maintenance in DLMDB.\nThese order statistics turn counting and sampling operations from O(n) to O(log\nn), cutting Datalog query planning time in half. This feature was introduced with\nminimal write overhead.</p>\n<h2>Query engine</h2>\n<p>Datalevin's query planner performs extensive query rewrite passes to optimize\nperformance. For example, predicates are pushed down into index scans so\nfilters execute early rather than after joins; inequality predicates are\nrewritten into index range scans, and so on.</p>\n<p>The planner simplifies the query graph by treating stars as meta-nodes, then\napplies a Selinger-style dynamic programming algorithm with accurate cardinality\nestimates. Merge scans collapse star-shaped entity access into a single scan,\navoiding redundant joins for attributes belonging to the same entity group. More\ndetails on these optimizations can be found in the <a href=\"https://github.com/juji-io/datalevin/blob/master/doc/query.md\">query engine\ndocumentation</a>.</p>\n<p>Even with relatively accurate cardinality estimation, occasional bad plans are\nunavoidable, particularly for tricky joins that link different\nentities. To reduce the impact of such cases, the cost-based optimizer now\nconsiders hash joins as an alternative when input size is large. We also\nextended the optimizer's coverage to more complex query clauses, such as\n<code>or-join</code>.</p>\n<p>We made the multi-threaded query execution pipeline more robust by handling edge\ncases in concurrency and adding backpressure. The pipeline now uses its own\nthread pool to avoid contention with worker thread pools.</p>\n<p>Other improvements have focused on usability: \"creature-comfort\" features like\na <code>:having</code> clause, allowing math expressions in <code>:find</code>, specifying sort\nvariables by indices in <code>:order-by</code>, full boolean search expressions for\nfull-text, and so on. These features reduce the amount of custom code needed for\npost-processing results, and in-database operations are usually more efficient\nthan equivalent work in application code.</p>\n<h2>Rule engine</h2>\n<p>Datalevin uses rules to bundle a set of query clauses into named, reusable\ninvocations. As rules can call themselves and other rules, this feature enables\nrecursive logic computation and graph navigation. We wrote a new rule engine\nthat leverages the same cost-based optimizer, allowing Datalevin to serve as an\nefficient graph database and logic reasoner.</p>\n<p>The rule engine uses semi-naive fixpoint evaluation, applies magic-set rewrites\nwhere beneficial, and seeds evaluation from outer query bindings so it starts with\nrelevant candidates instead of a blank slate.</p>\n<p>In addition, non-recursive rule clauses are inlined into the main query to let\nthe optimizer plan them with index scans and joins. For T-stratified rules,\ntemporal elimination avoids storing unnecessary intermediate results. Detailed\ninformation is available in the <a href=\"https://github.com/juji-io/datalevin/blob/master/doc/rules.md\">rule engine\ndocumentation</a>.</p>\n<h2>Benchmarks</h2>\n<p>We added two new benchmarks to showcase the progress we have made.</p>\n<h3>Logic workloads: Math Genealogy benchmark</h3>\n<p>The <a href=\"https://github.com/juji-io/datalevin/tree/master/benchmarks/math-bench\">Math Genealogy\nbenchmark</a>\nfocuses entirely on rule resolution. It is a good stress test for recursive\nDatalog rules. The dataset contains roughly 256,769 dissertations, 256,767\nstudents, and 276,635 advisor-student relationships. There are four queries in\nthis benchmark. Datalevin's rule engine is very fast on these queries: Q1 (14.4\nms), Q2 (330.9 ms), Q3 (269.6 ms), and Q4 (recursive academic ancestry, 2.9 ms).</p>\n<p>By comparison, Datomic takes over 40 seconds on Q4, and Datascript runs out of\nmemory. This query is difficult because recursive ancestry computes a transitive\nclosure: each new level of ancestors can join with every previously found level,\nwhich can quickly create a combinatorial explosion of intermediate tuples. Even if\nthe average branching factor is modest (say b=3â€“5 advisors per student),\nintermediate results can grow on the order of b^k at depth k. If those tuples\nare generated repeatedly across branches, we end up materializing large\nintermediate relations just to discard most of them later.</p>\n<p>Why is Q4 so fast in Datalevin? It exploits bound starting points. The\nsemi-naive fixpoint evaluation works off delta relations only, i.e., each\niteration only joins newly produced tuples. When a query binds a head argument,\nthe engine also seeds recursion (and, when effective, applies magic-set\nrewrites) so it only explores the reachable slice of the graph rather than\nmaterializing the full closure.</p>\n<h3>Graph workloads: LDBC SNB benchmark</h3>\n<p>Graph workloads are where triple stores should shine, but performance is where\ndedicated graph databases usually try to defend their turf. The <a href=\"https://ldbcouncil.org/benchmarks/snb/\">LDBC Social\nNetwork Benchmark (SNB)</a> is an\nindustry-standard benchmark for interactive graph queries. We implemented the\nfull workload and included a Neo4j implementation for comparison\n<a href=\"https://github.com/juji-io/datalevin/tree/master/benchmarks/LDBC-SNB-bench\">here</a>.</p>\n<p>On the SF1 dataset (about 3.2M entities and 17.3M edges), Datalevin is 27x to\n620x faster on short interactive queries (pictured above), averaging 48x\nfaster than Neo4j. These short queries are the most commonly encountered\nworkloads in an operational graph database, so performing well on them has\nsignificant practical implications.</p>\n<p>Some of these short queries (IS2 and IS6) involve unbounded graph\ntraversal, such as finding the root post of a comment. Datalevin handles these\nwith a recursive rule. Thanks to the efficiency of our rule engine, graph\nnavigation performance is stellar.</p>\n<p>On complex queries, Datalevin is about 12% faster overall, with some\nqueries (IC6, IC8, IC11) orders of magnitude faster and a few (IC3, IC5, IC9)\nslower than Neo4j. I am sure Neo4j is extensively tuned for these queries, as it\nis one of the authors of this industry-standard benchmark. It is remarkable that\nDatalevin performs so well on these complex graph queries without any specific\ntuning.</p>\n<p>The important observation here is that the same triple store and query\nengine handle both relational-style joins and graph traversals without\nneeding special cases for either.</p>\n<h2>Towards the future</h2>\n<p>A triple store is a flexible substrate. When paired with a cost-based query\noptimizer and a modern rule engine, it can span relational, graph, and logical\nreasoning workloads. It can also expand toward richer document workloads\nwithout changing the underlying model.</p>\n<p>With the current focus on AI systems, a triple store like Datalevin can serve\nseveral critical purposes.</p>\n<p>An AI agent needs a context graph: entities, facts, relations, constraints, and\nmemories that evolve over time. Keeping that context in one integrated store\nreduces the impedance mismatch between relational tables, graph edges, embeddings,\nand documents. Datalevin makes retrieval and grounding first-class: full-text\nand vector search can pull candidate facts, while Datalog queries and rules can\nverify, connect, and constrain them. It can also support tool use, where the \"tool\noutputs\" are simply more facts to join and reason over. For example, in a RAG\npipeline, vector search retrieves candidate snippets, graph relations link\nthem to entities and events, and rules enforce constraints such as provenance or\nrecency.</p>\n<p>Datalevin can serve as an agent's memory model, where episodic facts,\nlong-term knowledge, and computed embeddings all live in one place and can be\nqueried together. In that sense, a unified store is to an AI agent what memory\nis to human cognition: the common ground where different kinds of signals meet\nto be reasoned over.</p>\n<p>Even as AI writes more code, a simple and versatile database remains highly\nAI-friendly. A context-limited LLM model benefits from a coherent data model and\na single query language that covers many use cases. Datalog is truly\ndeclarative, meaning there are fewer procedural or implementation details for a\nmodel to trip over. That translates to less boilerplate, fewer dialects to\nremember, and fewer quirks in query semantics, making it more likely that the\nsystem handles data correctly. In fact, AI wrote all the Datalevin queries used\nin the LDBC SNB benchmark mentioned above. Although it is still a relatively\nniche database, the AI composed queries for Datalevin with ease because the\nquery language is inherently simpler.</p>\n<h2>Next steps</h2>\n<p>The recent rule engine rewrite brings us much closer to the 1.0 release\nmilestone. With the addition of high availability, a JSON API, and libraries for\nother languages, we expect to reach this milestone this year.</p>\n<p>After six years of continuous research and development, Datalevin would not be\nin its current hardened state without the experimentation and production deployment\nefforts of the Clojure community. I truly appreciate everyone who has used Datalevin\nand made contributions. The future of simplified data storage and access is\nnear, for human and AI developers alike.</p>"}, :excerpts []}]